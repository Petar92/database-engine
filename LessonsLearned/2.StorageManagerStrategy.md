This summary was created using ChatGPT

# Storage Manager I/O Strategy

## Context

Originally, I considered the simplest route—opening and closing the database file on every command. This is a perfectly fine approach for a learning project but I wanted to learn more about the challenges of real-world systems where this kind of approach would introduce severe performance and scalability issues: repeated system calls, lack of shared buffering, and poor concurrency handling. To identify a more robust design suitable for production-grade systems, while looking for how this issue can be solved found the seminal paper by Hellerstein, Stonebraker, and Hamilton. A thing to note is that this paper is from 2007 and it might be outdated but I do not have enough knowledge on the topic to judge this.

## Discovering the Foundational Paper

While researching storage and I/O strategies, I came across [**“The Architecture of a Database System”**](https://db.cs.berkeley.edu/papers/fntdb07-architecture.pdf) by Joseph M. Hellerstein, Michael Stonebraker, and James Hamilton. I found this paper through a recommendation while studying buffer management and transactional storage modules in modern DBMS architectures.

## The Core Issue: Database File I/O

I needed to decide how my engine should access and manage the on-disk database file:

- **Re-open file per command** vs. **persistent open file**
- **Caching/Buffering** strategies to reduce disk I/O
- Ensuring consistency and performance in a multi-worker environment

## Options Presented in the Paper

The paper outlines the responsibilities of the storage manager, including:

- **Persistent file descriptors** kept open for each relation  
- A **shared buffer pool** for caching pages in memory  
- Transactional storage semantics (locks, logging) to ensure ACID  

It also describes three process models for handling concurrency:

1. **Process per DBMS Worker**: Easy isolation but heavy resource usage; requires shared memory for buffer pools  
2. **Thread per DBMS Worker**: Lightweight context switching but complex synchronization  
3. **Process Pool**: Fixed or dynamic pool of processes; balances overhead and isolation

## Chosen Approach: Process Pool with Persistent Files

I decided to implement a **Process Pool** and maintain a **persistent open file stream** in each worker process. This approach balances:

- **Performance**: Avoids repeated open/close overhead  
- **Isolation**: Each process has its own file descriptor and independent buffer state  
- **Resource Efficiency**: Limits the number of OS processes via a pool

## Pros and Cons of the Process Pool Approach

**Pros:**

- **Lower Latency**: Reusing open file descriptors reduces system call overhead  
- **Process Isolation**: Faults in one worker don’t affect others  
- **Controlled Concurrency**: Pool size limits resource usage under load

**Cons:**

- **IPC Complexity**: Requires robust inter-process communication for dispatching commands and retrieving results  
- **Error Handling**: Must detect and recover from crashed worker processes  
- **Cache Coherence**: Independent buffer state per process can lead to stale reads unless synchronized

## Next Steps

- Implement the process pool
- Build a lock manager and logging subsystem for transactional safety  
- Explore adding a shared-memory buffer pool for cross-process caching

